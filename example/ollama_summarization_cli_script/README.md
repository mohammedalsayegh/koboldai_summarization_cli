# Usage Guide for Ollama Summarization CLI Batch Script

## Description

This batch script automates the process of generating a summary from HTML files produced by the NoScribe software using the Ollama summarization tool. It integrates various command-line tools and requires the Ollama to be installed and configured with the Phi3 model.

## Requirements

Before using this script, ensure the following utilities and Ollama model are met:

- Install and run Ollama with the Phi3 model.
- **Compile the following command-line tools from this repository:**
  - json_text_merger.exe
  - koboldai_summarization_cli.exe
  - noscribe_transcript_extractor.exe
  - ollama_summarization_cli.exe
  - transcript-splitter.exe

## Usage

1. Place the HTML file(s) generated by the Noscribe software in the same directory as the `ollama_summarization_cli.bat` script.

2. Open a command prompt in the directory containing the batch script.

3. Run the batch script by typing its name and pressing Enter:

   ```batch
   ollama_summarization_cli.bat path\file.html
   ```

4. The script will prompt you with a warning message regarding the potentially unrevised nature of the operation. Respond with "yes" to continue or "no" to abort.

5. If you choose to continue, the script will proceed to execute the following steps:
   - Extract transcript from the HTML file using `noscribe_transcript_extractor.exe`.
   - Split the transcript into smaller segments using `transcript-splitter.exe`.
   - Summarize the segments using the Ollama summarization CLI.
   - Merge the summarized text using `json_text_merger.exe`.
   - Clean up temporary files.

6. After successful execution, the summarized output will be stored in the same directory as the input HTML file(s).

## Additional Notes

- Ensure that all required command-line tools are accessible in the system's PATH environment variable or in the same directory as the batch script.
- Make sure the Ollama summarization CLI is properly configured with the correct JSON file, as shown in the example, and that the Ollama backend server is running with the Phi3 model installed for accurate summarization.